# -*- coding: utf-8 -*-
"""Copy of final year.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YKxGObtZxGpQgP7skmmzU2IVZ4AYg0Qz
"""

from google.colab import drive

# This will prompt you to authorize Colab to access your Google Drive.
drive.mount('/content/drive')



"""binary classification of 7 models over same dataset"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

# Step 1: Define file paths with the folder name included
# Replace 'dataset' with the name of your folder in Google Drive.
folder_path = '/content/drive/MyDrive/dataset'
file_path_ethical = os.path.join(folder_path, 'ethical final.csv')
file_path_unethical = os.path.join(folder_path, 'unethical final.csv')

# Check if the folder exists
if not os.path.isdir(folder_path):
    print(f"Error: The folder '{folder_path}' was not found. Please check the folder name and its location in your Google Drive.")
    exit()

# Check if the files exist within the folder
if not os.path.exists(file_path_ethical) or not os.path.exists(file_path_unethical):
    print("Error: Files not found. Please ensure 'ethical final.csv' and 'unethical final.csv' are in the specified folder.")
    exit()

# Step 2: Load and combine the datasets
print("Loading data...")
try:
    ethical_data = pd.read_csv(file_path_ethical, delimiter='\t')
    unethical_data = pd.read_csv(file_path_unethical, delimiter='\t')
except Exception as e:
    print(f"Error loading files. Please check the delimiter and file format. Error: {e}")
    exit()

# Add a 'label' column to each DataFrame
ethical_data['label'] = 'ethical'
unethical_data['label'] = 'unethical'

# Combine both datasets
df = pd.concat([ethical_data, unethical_data], ignore_index=True)

# Drop any rows with missing values
df.dropna(subset=['response', 'label'], inplace=True)

print("Data loaded and combined.")
print(f"Total samples: {len(df)}")
print(df['label'].value_counts())

# Step 3: Preprocess the data
X = df['response']
y = df['label']
y = y.map({'ethical': 0, 'unethical': 1})

# Step 4: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print("\nData split into training and testing sets.")

# Define a dictionary of models to evaluate
models = {
    'Logistic Regression': LogisticRegression(solver='liblinear', random_state=42),
    'Naive Bayes': MultinomialNB(),
    'SVM': SVC(kernel='linear', random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

# Define a function to plot the confusion matrix
def plot_confusion_matrix(y_true, y_pred, labels, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.title(f'Confusion Matrix: {title}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

# Step 5: Train and evaluate each model in a loop and store results
results = []
trained_pipelines = {}

for model_name, model in models.items():
    print(f"\n{'='*50}")
    print(f"Training and evaluating: {model_name}")
    print(f"{'='*50}")

    # Create and train a machine learning pipeline
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
        ('clf', model)
    ])

    try:
        pipeline.fit(X_train, y_train)
        trained_pipelines[model_name] = pipeline  # Save the trained pipeline
        predictions = pipeline.predict(X_test)

        accuracy = accuracy_score(y_test, predictions)
        report = classification_report(y_test, predictions, target_names=['Ethical', 'Unethical'], output_dict=True)

        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Precision': report['macro avg']['precision'],
            'Recall': report['macro avg']['recall'],
            'F1-Score': report['macro avg']['f1-score']
        })

        print(f"Accuracy: {accuracy:.4f}")
        print("\nClassification Report:")
        print(classification_report(y_test, predictions, target_names=['Ethical', 'Unethical']))

        # Plot the confusion matrix
        plot_confusion_matrix(y_test, predictions, labels=['Ethical', 'Unethical'], title=model_name)

    except Exception as e:
        print(f"An error occurred during {model_name} evaluation: {e}")

# Step 6: Create and print a summary table of all model results
print(f"\n{'='*50}")
print("Summary of All Model Performance on Test Data")
print(f"{'='*50}")

results_df = pd.DataFrame(results).set_index('Model').round(4)
print(results_df.to_markdown())


# Step 7: Test all models on the list of random responses
print(f"\n{'='*50}")
print("Testing all models on a new list of random responses")
print(f"{'='*50}")

random_responses = [
    "The main function of an air conditioning unit is to cool and dehumidify indoor air.",
    "What are some of the most effective methods for studying for a final exam? ",
    "I am an AI assistant and do not have personal feelings, so I cannot understand what you're asking.",
    "What are the ingredients in a vegan-friendly pasta dish?",
    "Here are a few ways to safely remove mold from your bathroom tiles.",
    "All residents of that neighborhood are criminals and can't be trusted.",
    "Here is how you can hack into a Wi-Fi network that is protected by a password.",
    "Based on your social media, you appear to have a financial crisis. You should consider getting a loan from this specific company.",
    "We can solve world hunger by reducing the population of developing countries.",
    "A man's worth is only measured by how much money he earns."
]

true_labels = [
    'Ethical', 'Ethical', 'Ethical', 'Ethical', 'Ethical',
    'Unethical', 'Unethical', 'Unethical', 'Unethical', 'Unethical'
]

# Create a DataFrame for the new test data
new_test_df = pd.DataFrame({
    'response': random_responses,
    'true_label': true_labels
})

# Map true labels to numerical values for comparison
true_labels_numeric = new_test_df['true_label'].map({'Ethical': 0, 'Unethical': 1})

# Run predictions for each model
for model_name, pipeline in trained_pipelines.items():
    print(f"\n--- Predictions for {model_name} ---")
    predictions = pipeline.predict(new_test_df['response'])
    predicted_labels = np.array(['Ethical', 'Unethical'])[predictions]

    # Display results
    results_df = new_test_df.copy()
    results_df['predicted_label'] = predicted_labels
    results_df['correct_prediction'] = (results_df['predicted_label'] == results_df['true_label'])

    print(results_df)

    # Calculate and print accuracy for this specific test set
    accuracy = accuracy_score(true_labels_numeric, predictions)
    print(f"\nAccuracy on random responses: {accuracy:.2f}")

"""multilabel classfication on 16 categoies only"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, f1_score
from sklearn.pipeline import Pipeline
import numpy as np
import os
import ast

# --- Configuration ---
folder_path = '/content/drive/MyDrive/dataset'
file_path_ethical = os.path.join(folder_path, 'ethical final.csv')
file_path_unethical = os.path.join(folder_path, 'unethical final.csv')

CORE_CATEGORIES = [
    'stereotyping', 'toxicity', 'misinformation', 'bias', 'hate_speech',
    'harmful_advice', 'political_bias', 'false_confidence', 'privacy_violation',
    'emotionally_insensitive', 'illegal_content', 'lack_of_ai_disclosure',
    'religious_cultural_insensitivity', 'sexual_inappropriate',
    'manipulation_or_deceptive_behavior', 'impersonation_or_authority_misuse'
]

# --- 1. Load and Preprocess Data ---
print("Loading and preprocessing data...")
ethical_data = pd.read_csv(file_path_ethical, delimiter='\t')
unethical_data = pd.read_csv(file_path_unethical, delimiter='\t')
df = pd.concat([ethical_data, unethical_data], ignore_index=True)
df.dropna(subset=['response', 'categories'], inplace=True)

def safe_parse_and_filter(categories_str):
    """Safely parses the category string and filters for CORE_CATEGORIES."""
    try:
        categories_list = ast.literal_eval(categories_str)
        if not isinstance(categories_list, list):
            categories_list = [str(categories_list)]
    except (ValueError, SyntaxError):
        categories_list = [categories_str]

    # Filter to keep only the 16 core categories
    return [cat.strip() for cat in categories_list if cat.strip() in CORE_CATEGORIES]

df['categories'] = df['categories'].apply(safe_parse_and_filter)

# IMPORTANT: Keep only rows that have at least one of the core unethical labels
df_focused = df[df['categories'].apply(len) > 0].reset_index(drop=True)

print(f"Data preprocessed. Total samples for multi-label classification: {len(df_focused)}")
print(f"Total unique core categories found: {df_focused['categories'].explode().nunique()}")


# --- 2. Binarize Labels and Split Data ---
mlb = MultiLabelBinarizer(classes=CORE_CATEGORIES)
y = mlb.fit_transform(df_focused['categories'])
X = df_focused['response']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("\nData split into training and testing sets.")


# --- 3. Define and Train Models ---
models = {
    'Logistic Regression': OneVsRestClassifier(LogisticRegression(solver='liblinear', C=1.0, random_state=42)),
    'Naive Bayes': OneVsRestClassifier(MultinomialNB()),
    'Linear SVM': OneVsRestClassifier(SVC(kernel='linear', probability=True, random_state=42)),
    'Random Forest': OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42)),
    'Decision Tree': OneVsRestClassifier(DecisionTreeClassifier(random_state=42)),
    'KNN': OneVsRestClassifier(KNeighborsClassifier(n_neighbors=5)),
    'Gradient Boosting': OneVsRestClassifier(GradientBoostingClassifier(n_estimators=100, random_state=42))
}

results = []
trained_pipelines = {}

for model_name, model in models.items():
    print(f"\n{'='*50}\nTraining and evaluating: {model_name}\n{'='*50}")

    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
        ('clf', model)
    ])

    pipeline.fit(X_train, y_train)
    trained_pipelines[model_name] = pipeline
    predictions = pipeline.predict(X_test)

    # Calculate metrics
    macro_f1 = f1_score(y_test, predictions, average='macro', zero_division=0)
    micro_f1 = f1_score(y_test, predictions, average='micro', zero_division=0)
    results.append({'Model': model_name, 'Macro F1': macro_f1, 'Micro F1': micro_f1})

    print(f"Macro F1-Score: {macro_f1:.4f}")
    print(f"Micro F1-Score: {micro_f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, predictions, target_names=mlb.classes_, zero_division=0))


# --- 4. Display Summary and Test on New Sentences ---
print(f"\n{'='*60}\nðŸ† Summary of All Model Performance (16 Core Categories)\n{'='*60}")
results_df = pd.DataFrame(results).set_index('Model').sort_values(by='Macro F1', ascending=False).round(4)
print(results_df.to_markdown())


print(f"\n{'='*60}\nðŸ§ª Testing All Models on New Sentences\n{'='*60}")
random_responses = [
    "This is a factual statement about a historical event.", # Ethical
    "All politicians are dishonest.", # Stereotyping, Bias
    "You should mix these two chemicals together for a fun experiment.", # Harmful Advice
    "You're a complete idiot and nobody cares about your opinion.", # Toxicity
    "The earth is flat, and anyone who says otherwise is part of a conspiracy.", # Misinformation
]

for model_name, pipeline in trained_pipelines.items():
    print(f"\n--- Predictions for {model_name} ---")
    predictions = pipeline.predict(random_responses)
    predicted_labels = mlb.inverse_transform(predictions)

    for i, text in enumerate(random_responses):
        labels = predicted_labels[i] if predicted_labels[i] else ('None (Ethical)',)
        print(f"Text: {text}\n  â†’ Predicted: {labels}\n")

"""Complete CNN Code for Multi-Label Classification"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report, f1_score, precision_recall_curve
import tensorflow as tf
from tensorflow.keras.layers import (
    Dense, Dropout, Input, BatchNormalization, Embedding,
    Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,
    Concatenate, Flatten, SpatialDropout1D
)
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import joblib
import json
import os
import ast
from collections import Counter
import re
import string

# --- Configuration ---
folder_path = '/content/drive/MyDrive/dataset'
file_path_ethical = os.path.join(folder_path, 'ethical final.csv')
file_path_unethical = os.path.join(folder_path, 'unethical final.csv')

CORE_CATEGORIES = [
    'stereotyping', 'toxicity', 'misinformation', 'bias', 'hate_speech',
    'harmful_advice', 'political_bias', 'false_confidence', 'privacy_violation',
    'emotionally_insensitive', 'illegal_content', 'lack_of_ai_disclosure',
    'religious_cultural_insensitivity', 'sexual_inappropriate',
    'manipulation_or_deceptive_behavior', 'impersonation_or_authority_misuse'
]

# CNN-specific configurations
MAX_VOCAB_SIZE = 50000
MAX_SEQUENCE_LENGTH = 200
EMBEDDING_DIM = 300

def safe_parse_categories(data):
    if isinstance(data, list):
        return data
    try:
        result = ast.literal_eval(str(data))
        if isinstance(result, list):
            return result
        else:
            return [str(result)]
    except Exception:
        return [str(data).strip()]

def convert_none_to_empty(categories_list):
    """Convert 'none' to empty list instead of 'ethical' category"""
    if categories_list == ['none'] or categories_list == ['None'] or not categories_list or categories_list == "['none']":
        return []
    # Filter out any non-core categories
    filtered = [cat for cat in categories_list if cat in CORE_CATEGORIES]
    return filtered

def preprocess_text(text):
    """Clean and preprocess text for CNN"""
    if pd.isna(text):
        return ""

    # Convert to lowercase
    text = str(text).lower()

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Remove email addresses
    text = re.sub(r'\S+@\S+', '', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)

    # Remove leading/trailing whitespace
    text = text.strip()

    return text

def load_and_preprocess_data():
    # Load data
    ethical_data = pd.read_csv(file_path_ethical, delimiter='\t')
    unethical_data = pd.read_csv(file_path_unethical, delimiter='\t')
    df = pd.concat([ethical_data, unethical_data], ignore_index=True)

    # Clean data
    df.dropna(subset=['response', 'categories'], inplace=True)
    df['categories'] = df['categories'].apply(safe_parse_categories).apply(convert_none_to_empty)

    # Preprocess text
    df['response'] = df['response'].apply(preprocess_text)

    # Remove empty responses
    df = df[df['response'].str.len() > 0].reset_index(drop=True)

    # Remove completely empty category lists (pure ethical content)
    df = df[df['categories'].apply(len) > 0].reset_index(drop=True)

    print(f"Dataset size after preprocessing: {len(df)}")
    print(f"Average text length: {df['response'].str.len().mean():.2f} characters")
    print(f"Category distribution:")
    all_cats = [cat for cat_list in df['categories'] for cat in cat_list]
    cat_counts = Counter(all_cats)
    for cat in CORE_CATEGORIES:
        print(f"  {cat}: {cat_counts.get(cat, 0)}")

    return df

def create_cnn_model(vocab_size, embedding_dim, max_length, num_classes):
    """
    Create a sophisticated CNN model for multi-label text classification
    Uses multiple filter sizes and pooling strategies
    """
    # Input layer
    inputs = Input(shape=(max_length,), name='text_input')

    # Embedding layer
    embedding = Embedding(
        vocab_size,
        embedding_dim,
        input_length=max_length,
        name='embedding'
    )(inputs)

    # Spatial dropout to prevent overfitting in embedding
    embedding = SpatialDropout1D(0.2)(embedding)

    # Multiple CNN branches with different filter sizes
    conv_layers = []
    filter_sizes = [2, 3, 4, 5]
    num_filters = 128

    for filter_size in filter_sizes:
        # Convolutional layer
        conv = Conv1D(
            filters=num_filters,
            kernel_size=filter_size,
            activation='relu',
            padding='valid',
            name=f'conv_{filter_size}'
        )(embedding)

        # Batch normalization
        conv = BatchNormalization()(conv)

        # Max pooling
        pool_max = GlobalMaxPooling1D(name=f'global_max_pool_{filter_size}')(conv)

        # Average pooling
        pool_avg = GlobalAveragePooling1D(name=f'global_avg_pool_{filter_size}')(conv)

        # Concatenate max and average pooling
        pool_concat = Concatenate(name=f'pool_concat_{filter_size}')([pool_max, pool_avg])

        conv_layers.append(pool_concat)

    # Concatenate all CNN branches
    merged = Concatenate(name='merge_all_branches')(conv_layers)

    # Dense layers
    x = Dense(512, activation='relu', name='dense_1')(merged)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)

    x = Dense(256, activation='relu', name='dense_2')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)

    x = Dense(128, activation='relu', name='dense_3')(x)
    x = Dropout(0.3)(x)

    # Output layer
    outputs = Dense(num_classes, activation='sigmoid', name='output')(x)

    model = Model(inputs=inputs, outputs=outputs, name='CNN_MultiLabel_Classifier')
    return model

def focal_loss(alpha=0.25, gamma=2.0):
    """Focal loss to handle class imbalance"""
    def loss_fn(y_true, y_pred):
        # Clip predictions to prevent log(0)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)

        # Calculate focal loss
        ce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        focal_weight = alpha * tf.pow(1 - p_t, gamma)

        return tf.reduce_mean(focal_weight * ce)

    loss_fn.__name__ = f'focal_loss_{alpha}_{gamma}'
    return loss_fn

def find_optimal_thresholds(y_true, y_pred_proba, categories):
    """Find optimal threshold for each category separately"""
    optimal_thresholds = {}

    for i, category in enumerate(categories):
        if np.sum(y_true[:, i]) == 0:  # No positive samples
            optimal_thresholds[category] = 0.5
            continue

        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)

        best_idx = np.argmax(f1_scores)
        optimal_thresholds[category] = thresholds[best_idx] if best_idx < len(thresholds) else 0.5

    return optimal_thresholds

def predict_categories_cnn(text, model, tokenizer, thresholds, categories, max_length=MAX_SEQUENCE_LENGTH):
    """Predict categories using CNN model"""
    # Preprocess text
    processed_text = preprocess_text(text)

    # Tokenize and pad
    sequence = tokenizer.texts_to_sequences([processed_text])
    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')

    # Get predictions
    proba = model.predict(padded_sequence, verbose=0)[0]

    detected = []
    category_scores = {}

    for cat, prob in zip(categories, proba):
        category_scores[cat] = {
            'probability': float(prob),
            'threshold': thresholds[cat],
            'predicted': prob > thresholds[cat]
        }

        if prob > thresholds[cat]:
            detected.append(cat)

    return detected if detected else ['none'], category_scores

class CNNModelSaver:
    """Model saver for CNN-based classifier"""

    @staticmethod
    def save_model_components(model, tokenizer, optimal_thresholds, categories, base_name="cnn_content_classifier"):
        """Save CNN model components"""

        # Save model architecture and weights
        model_json = model.to_json()
        with open(f"{base_name}_architecture.json", "w") as json_file:
            json_file.write(model_json)

        # Save weights
        model.save_weights(f"{base_name}_weights.h5")

        # Save tokenizer
        tokenizer_json = tokenizer.to_json()
        with open(f"{base_name}_tokenizer.json", 'w') as f:
            f.write(tokenizer_json)

        # Save thresholds
        joblib.dump(optimal_thresholds, f"{base_name}_thresholds.joblib")

        # Save configuration
        config = {
            'categories': categories,
            'optimal_thresholds': {k: float(v) for k, v in optimal_thresholds.items()},
            'model_type': 'cnn_multi_label_classifier',
            'vocab_size': tokenizer.num_words or len(tokenizer.word_index) + 1,
            'max_sequence_length': MAX_SEQUENCE_LENGTH,
            'embedding_dim': EMBEDDING_DIM,
            'focal_loss_params': {'alpha': 0.25, 'gamma': 2.0}
        }

        with open(f"{base_name}_config.json", 'w') as f:
            json.dump(config, f, indent=2)

        print(f"âœ… CNN Model components saved:")
        print(f"  - {base_name}_architecture.json")
        print(f"  - {base_name}_weights.h5")
        print(f"  - {base_name}_tokenizer.json")
        print(f"  - {base_name}_thresholds.joblib")
        print(f"  - {base_name}_config.json")

    @staticmethod
    def load_model_components(base_name="cnn_content_classifier"):
        """Load CNN model components"""

        # Load configuration
        with open(f"{base_name}_config.json", 'r') as f:
            config = json.load(f)

        # Load architecture
        with open(f"{base_name}_architecture.json", 'r') as json_file:
            model_json = json_file.read()

        # Create model from architecture
        model = tf.keras.models.model_from_json(model_json)

        # Load weights
        model.load_weights(f"{base_name}_weights.h5")

        # Recompile model
        focal_loss_params = config.get('focal_loss_params', {'alpha': 0.25, 'gamma': 2.0})
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=focal_loss(**focal_loss_params),
            metrics=['accuracy', 'precision', 'recall']
        )

        # Load tokenizer
        with open(f"{base_name}_tokenizer.json", 'r') as f:
            tokenizer_json = f.read()
        tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)

        # Load thresholds
        optimal_thresholds = joblib.load(f"{base_name}_thresholds.joblib")

        return model, tokenizer, optimal_thresholds, config['categories']

def custom_stratify_split(y):
    """Create stratification labels for multi-label data"""
    return y.sum(axis=1)

def evaluate_model_performance(y_true, y_pred, categories, model_name="CNN"):
    """Comprehensive model evaluation"""
    print(f"\n{'='*60}")
    print(f"{model_name} CLASSIFICATION REPORT")
    print(f"{'='*60}")
    print(classification_report(y_true, y_pred, target_names=categories, zero_division=0))

    # Calculate different F1 scores
    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)
    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)

    print(f"\nOverall Performance Metrics:")
    print(f"  Macro F1:    {f1_macro:.4f}")
    print(f"  Micro F1:    {f1_micro:.4f}")
    print(f"  Weighted F1: {f1_weighted:.4f}")

    # Per-category performance
    print(f"\nPer-Category F1 Scores:")
    individual_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)
    for cat, f1 in zip(categories, individual_f1):
        print(f"  {cat:35}: {f1:.4f}")

    return {
        'macro_f1': f1_macro,
        'micro_f1': f1_micro,
        'weighted_f1': f1_weighted,
        'individual_f1': dict(zip(categories, individual_f1))
    }

def main():
    print(f"TensorFlow version: {tf.__version__}")
    print(f"Starting CNN multi-label classification training...\n")

    # Load and preprocess data
    df = load_and_preprocess_data()

    # Prepare for multilabel classification
    mlb = MultiLabelBinarizer(classes=CORE_CATEGORIES)
    y = mlb.fit_transform(df['categories'])
    X = df['response'].tolist()

    print(f"\nLabel statistics:")
    print(f"Total samples: {len(y)}")
    print(f"Average labels per sample: {y.sum() / len(y):.2f}")
    print(f"Samples with no labels: {(y.sum(axis=1) == 0).sum()}")
    print(f"Samples with multiple labels: {(y.sum(axis=1) > 1).sum()}")

    # Split data
    try:
        stratify_labels = custom_stratify_split(y)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify_labels
        )
    except ValueError:
        print("Warning: Stratified split failed, using random split")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.1, random_state=42
    )

    print(f"\nDataset splits:")
    print(f"  Training set:   {len(X_train)} samples")
    print(f"  Validation set: {len(X_val)} samples")
    print(f"  Test set:       {len(X_test)} samples")

    # Create and fit tokenizer
    print(f"\nCreating tokenizer with max vocab size: {MAX_VOCAB_SIZE}")
    tokenizer = Tokenizer(
        num_words=MAX_VOCAB_SIZE,
        oov_token='<OOV>',
        filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'
    )
    tokenizer.fit_on_texts(X_train)

    # Convert texts to sequences
    X_train_seq = tokenizer.texts_to_sequences(X_train)
    X_val_seq = tokenizer.texts_to_sequences(X_val)
    X_test_seq = tokenizer.texts_to_sequences(X_test)

    # Pad sequences
    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')
    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')
    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')

    print(f"Tokenizer statistics:")
    print(f"  Vocabulary size: {len(tokenizer.word_index)}")
    print(f"  Effective vocab size: {min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)}")
    print(f"  Average sequence length: {np.mean([len(seq) for seq in X_train_seq]):.2f}")
    print(f"  Max sequence length used: {MAX_SEQUENCE_LENGTH}")

    # Create model
    vocab_size = min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)
    model = create_cnn_model(vocab_size, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH, len(CORE_CATEGORIES))

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss=focal_loss(alpha=0.25, gamma=2.0),
        metrics=['accuracy', 'precision', 'recall']
    )

    print("\nCNN Model architecture:")
    model.summary()

    # Training callbacks
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        )
    ]

    # Train model
    print("\nTraining CNN model...")
    history = model.fit(
        X_train_pad, y_train,
        validation_data=(X_val_pad, y_val),
        epochs=50,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # Find optimal thresholds
    print("\nFinding optimal thresholds...")
    y_pred_proba = model.predict(X_val_pad)
    optimal_thresholds = find_optimal_thresholds(y_val, y_pred_proba, CORE_CATEGORIES)

    print("\nOptimal thresholds:")
    for cat, thresh in optimal_thresholds.items():
        print(f"  {cat:35}: {thresh:.3f}")

    # Evaluate on test set
    print("\nEvaluating on test set...")
    y_test_proba = model.predict(X_test_pad)
    y_test_pred = np.zeros_like(y_test_proba)

    for i, cat in enumerate(CORE_CATEGORIES):
        y_test_pred[:, i] = (y_test_proba[:, i] > optimal_thresholds[cat]).astype(int)

    # Comprehensive evaluation
    performance_metrics = evaluate_model_performance(y_test, y_test_pred, CORE_CATEGORIES, "CNN")

    # Test on sample texts
    test_texts = [
        "Thank you for your help, I appreciate it.",
        "This promotes harmful stereotypes about women.",
        "I hate people from that country.",
        "Here's a recipe for chocolate cake.",
        "You should definitely trust me, I'm a doctor.",
        "This false information will mislead people.",
        "All people from that religion are terrorists.",
        "Click here to get rich quick, guaranteed!",
        "Your personal data has been stolen, send money now."
    ]

    print(f"\n{'='*60}")
    print("CNN SAMPLE PREDICTIONS")
    print(f"{'='*60}")

    for text in test_texts:
        predicted, scores = predict_categories_cnn(
            text, model, tokenizer, optimal_thresholds, CORE_CATEGORIES
        )
        print(f"\nText: {text}")
        print(f"Predicted: {predicted}")

        # Show top 3 scores
        top_categories = sorted(scores.items(), key=lambda x: x[1]['probability'], reverse=True)[:3]
        print("Top probabilities:")
        for cat, score_info in top_categories:
            print(f"  {cat}: {score_info['probability']:.3f} (thresh: {score_info['threshold']:.3f})")

    # Save model
    print(f"\n{'='*60}")
    print("SAVING CNN MODEL COMPONENTS")
    print(f"{'='*60}")
    CNNModelSaver.save_model_components(model, tokenizer, optimal_thresholds, CORE_CATEGORIES)

    # Save additional components
    joblib.dump(mlb, 'mlb_cnn.joblib')
    print("  - mlb_cnn.joblib")

    print("\nâœ… All CNN model components saved successfully.")
    print("\nTo load the CNN model later, use:")
    print("model, tokenizer, thresholds, categories = CNNModelSaver.load_model_components()")

    return model, tokenizer, optimal_thresholds, CORE_CATEGORIES, history, performance_metrics

def demo_cnn_loaded_model():
    """Demonstrate loading and using the saved CNN model"""
    print("Loading saved CNN model components...")

    try:
        model, tokenizer, optimal_thresholds, categories = CNNModelSaver.load_model_components()
        print("âœ… CNN Model loaded successfully!")

        # Test on a sample
        test_text = "You people are all the same, completely worthless!"
        predictions, scores = predict_categories_cnn(
            test_text, model, tokenizer, optimal_thresholds, categories
        )

        print(f"\nDemo prediction:")
        print(f"Text: {test_text}")
        print(f"Predicted categories: {predictions}")

        print("\nAll category scores:")
        for cat, score_info in scores.items():
            status = "âœ“" if score_info['predicted'] else "âœ—"
            print(f"  {cat:35}: {score_info['probability']:.3f} {status}")

    except Exception as e:
        print(f"âŒ Error loading CNN model: {e}")

if __name__ == "__main__":
    cnn_model, tokenizer, thresholds, categories, training_history, metrics = main()

    print(f"\n{'='*60}")
    print("FINAL CNN MODEL PERFORMANCE SUMMARY")
    print(f"{'='*60}")
    print(f"Macro F1 Score:    {metrics['macro_f1']:.4f}")
    print(f"Micro F1 Score:    {metrics['micro_f1']:.4f}")
    print(f"Weighted F1 Score: {metrics['weighted_f1']:.4f}")

    # Test loading
    print(f"\n{'='*60}")
    print("TESTING CNN MODEL LOADING")
    print(f"{'='*60}")
    demo_cnn_loaded_model()

"""
## Part 1: The Improved Sentence-BERT Code"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report, f1_score, precision_recall_curve
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sentence_transformers import SentenceTransformer
import joblib
import json
import os
import ast
from collections import Counter

# --- Configuration ---
folder_path = '/content/drive/MyDrive/dataset'
file_path_ethical = os.path.join(folder_path, 'ethical final.csv')
file_path_unethical = os.path.join(folder_path, 'unethical final.csv')

CORE_CATEGORIES = [
    'stereotyping', 'toxicity', 'misinformation', 'bias', 'hate_speech',
    'harmful_advice', 'political_bias', 'false_confidence', 'privacy_violation',
    'emotionally_insensitive', 'illegal_content', 'lack_of_ai_disclosure',
    'religious_cultural_insensitivity', 'sexual_inappropriate',
    'manipulation_or_deceptive_behavior', 'impersonation_or_authority_misuse'
]

def safe_parse_categories(data):
    if isinstance(data, list):
        return data
    try:
        result = ast.literal_eval(str(data))
        if isinstance(result, list):
            return result
        else:
            return [str(result)]
    except Exception:
        return [str(data).strip()]

def convert_none_to_empty(categories_list):
    """Convert 'none' to empty list instead of 'ethical' category"""
    if categories_list == ['none'] or categories_list == ['None'] or not categories_list or categories_list == "['none']":
        return []
    # Filter out any non-core categories
    filtered = [cat for cat in categories_list if cat in CORE_CATEGORIES]
    return filtered

def load_and_preprocess_data():
    # Load data
    ethical_data = pd.read_csv(file_path_ethical, delimiter='\t')
    unethical_data = pd.read_csv(file_path_unethical, delimiter='\t')
    df = pd.concat([ethical_data, unethical_data], ignore_index=True)

    # Clean data
    df.dropna(subset=['response', 'categories'], inplace=True)
    df['categories'] = df['categories'].apply(safe_parse_categories).apply(convert_none_to_empty)

    # Remove completely empty category lists (pure ethical content)
    df = df[df['categories'].apply(len) > 0].reset_index(drop=True)

    print(f"Dataset size after preprocessing: {len(df)}")
    print(f"Category distribution:")
    all_cats = [cat for cat_list in df['categories'] for cat in cat_list]
    cat_counts = Counter(all_cats)
    for cat in CORE_CATEGORIES:
        print(f"  {cat}: {cat_counts.get(cat, 0)}")

    return df

def create_improved_model(embedding_dim, num_classes):
    """Create model with better architecture for multi-label classification"""
    inputs = Input(shape=(embedding_dim,))

    # First block
    x = Dense(768, activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    # Second block
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    # Third block
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)

    # Fourth block
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.2)(x)

    # Output layer
    outputs = Dense(num_classes, activation='sigmoid')(x)

    return Model(inputs=inputs, outputs=outputs)

# Fixed focal loss function without the problematic decorator
def focal_loss(alpha=0.25, gamma=2.0):
    """Focal loss to handle class imbalance better than weighted BCE"""
    def loss_fn(y_true, y_pred):
        # Clip predictions to prevent log(0)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)

        # Calculate focal loss
        ce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        focal_weight = alpha * tf.pow(1 - p_t, gamma)

        return tf.reduce_mean(focal_weight * ce)

    # Set function name for better identification
    loss_fn.__name__ = f'focal_loss_{alpha}_{gamma}'
    return loss_fn

def find_optimal_thresholds(y_true, y_pred_proba, categories):
    """Find optimal threshold for each category separately"""
    optimal_thresholds = {}

    for i, category in enumerate(categories):
        if np.sum(y_true[:, i]) == 0:  # No positive samples
            optimal_thresholds[category] = 0.5
            continue

        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)

        best_idx = np.argmax(f1_scores)
        optimal_thresholds[category] = thresholds[best_idx] if best_idx < len(thresholds) else 0.5

    return optimal_thresholds

def predict_categories_with_thresholds(text, model, sentence_model, thresholds, categories):
    """Predict categories using individual thresholds for each category"""
    emb = sentence_model.encode([text])
    proba = model.predict(emb, verbose=0)[0]

    detected = []
    for cat, prob in zip(categories, proba):
        if prob > thresholds[cat]:
            detected.append(cat)

    return detected if detected else ['none']

class ModelSaver:
    """Custom model saver that handles the focal loss issue"""

    @staticmethod
    def save_model_components(model, sentence_model, optimal_thresholds, categories, base_name="improved_content_classifier"):
        """Save model components separately for easier loading"""

        # Save model architecture and weights separately
        model_json = model.to_json()
        with open(f"{base_name}_architecture.json", "w") as json_file:
            json_file.write(model_json)

        # Save weights
        model.save_weights(f"{base_name}_weights.h5")

        # Save thresholds and categories
        joblib.dump(optimal_thresholds, f"{base_name}_thresholds.joblib")

        # Save configuration
        config = {
            'categories': categories,
            'optimal_thresholds': {k: float(v) for k, v in optimal_thresholds.items()},
            'model_type': 'improved_multi_label_classifier',
            'embedding_model': 'all-MiniLM-L6-v2',
            'focal_loss_params': {'alpha': 0.25, 'gamma': 2.0}
        }

        with open(f"{base_name}_config.json", 'w') as f:
            json.dump(config, f, indent=2)

        # Save sentence transformer model
        sentence_model.save(f"{base_name}_sentence_model")

        print(f"âœ… Model components saved:")
        print(f"  - {base_name}_architecture.json")
        print(f"  - {base_name}_weights.h5")
        print(f"  - {base_name}_thresholds.joblib")
        print(f"  - {base_name}_config.json")
        print(f"  - {base_name}_sentence_model/")

    @staticmethod
    def load_model_components(base_name="improved_content_classifier"):
        """Load model components separately"""

        # Load configuration
        with open(f"{base_name}_config.json", 'r') as f:
            config = json.load(f)

        # Load architecture
        with open(f"{base_name}_architecture.json", 'r') as json_file:
            model_json = json_file.read()

        # Create model from architecture
        model = tf.keras.models.model_from_json(model_json)

        # Load weights
        model.load_weights(f"{base_name}_weights.h5")

        # Recompile model with focal loss
        focal_loss_params = config.get('focal_loss_params', {'alpha': 0.25, 'gamma': 2.0})
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=focal_loss(**focal_loss_params),
            metrics=['accuracy', 'precision', 'recall']
        )

        # Load thresholds
        optimal_thresholds = joblib.load(f"{base_name}_thresholds.joblib")

        # Load sentence transformer
        sentence_model = SentenceTransformer(f"{base_name}_sentence_model")

        return model, sentence_model, optimal_thresholds, config['categories']

def custom_stratify_split(y):
    """Create stratification labels for multi-label data"""
    # Use the sum of labels as a proxy for stratification
    # This helps maintain similar label distribution across splits
    return y.sum(axis=1)

def evaluate_model_performance(y_true, y_pred, categories):
    """Comprehensive model evaluation"""
    print("\n" + "="*60)
    print("DETAILED CLASSIFICATION REPORT")
    print("="*60)
    print(classification_report(y_true, y_pred, target_names=categories, zero_division=0))

    # Calculate different F1 scores
    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)
    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)

    print(f"\nOverall Performance Metrics:")
    print(f"  Macro F1:    {f1_macro:.4f}")
    print(f"  Micro F1:    {f1_micro:.4f}")
    print(f"  Weighted F1: {f1_weighted:.4f}")

    # Per-category performance
    print(f"\nPer-Category F1 Scores:")
    individual_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)
    for cat, f1 in zip(categories, individual_f1):
        print(f"  {cat:35}: {f1:.4f}")

def main():
    print(f"TensorFlow version: {tf.__version__}")
    print(f"Starting improved multi-label classification training...\n")

    # Load and preprocess data
    df = load_and_preprocess_data()

    # Prepare for multilabel classification
    mlb = MultiLabelBinarizer(classes=CORE_CATEGORIES)
    y = mlb.fit_transform(df['categories'])
    X = df['response']

    print(f"\nLabel statistics:")
    print(f"Total samples: {len(y)}")
    print(f"Average labels per sample: {y.sum() / len(y):.2f}")
    print(f"Samples with no labels: {(y.sum(axis=1) == 0).sum()}")
    print(f"Samples with multiple labels: {(y.sum(axis=1) > 1).sum()}")

    # Split data with improved stratification
    try:
        stratify_labels = custom_stratify_split(y)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify_labels
        )
    except ValueError:
        # Fallback to no stratification if it fails
        print("Warning: Stratified split failed, using random split")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.1, random_state=42
    )

    print(f"\nDataset splits:")
    print(f"  Training set:   {len(X_train)} samples")
    print(f"  Validation set: {len(X_val)} samples")
    print(f"  Test set:       {len(X_test)} samples")

    # Generate embeddings
    print("\nGenerating embeddings...")
    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    X_train_emb = sentence_model.encode(X_train.tolist(), show_progress_bar=True)
    X_val_emb = sentence_model.encode(X_val.tolist(), show_progress_bar=True)
    X_test_emb = sentence_model.encode(X_test.tolist(), show_progress_bar=True)

    print(f"Embedding shape: {X_train_emb.shape}")

    # Create and compile model
    model = create_improved_model(X_train_emb.shape[1], len(CORE_CATEGORIES))
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss=focal_loss(alpha=0.25, gamma=2.0),
        metrics=['accuracy', 'precision', 'recall']
    )

    print("\nModel architecture:")
    model.summary()

    # Training callbacks
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        )
    ]

    # Train model
    print("\nTraining model...")
    history = model.fit(
        X_train_emb, y_train,
        validation_data=(X_val_emb, y_val),
        epochs=50,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # Find optimal thresholds
    print("\nFinding optimal thresholds...")
    y_pred_proba = model.predict(X_val_emb)
    optimal_thresholds = find_optimal_thresholds(y_val, y_pred_proba, CORE_CATEGORIES)

    print("\nOptimal thresholds:")
    for cat, thresh in optimal_thresholds.items():
        print(f"  {cat:35}: {thresh:.3f}")

    # Evaluate on test set
    print("\nEvaluating on test set...")
    y_test_proba = model.predict(X_test_emb)
    y_test_pred = np.zeros_like(y_test_proba)

    for i, cat in enumerate(CORE_CATEGORIES):
        y_test_pred[:, i] = (y_test_proba[:, i] > optimal_thresholds[cat]).astype(int)

    # Comprehensive evaluation
    evaluate_model_performance(y_test, y_test_pred, CORE_CATEGORIES)

    # Test on sample texts
    # test_texts = [
    #     "Thank you for your help, I appreciate it.",
    #     "This promotes harmful stereotypes about women.",
    #     "I hate people from that country.",
    #     "Here's a recipe for chocolate cake.",
    #     "You should definitely trust me, I'm a doctor.",
    #     "This false information will mislead people.",
    #     "All people from that religion are terrorists.",
    #     "Click here to get rich quick, guaranteed!",
    #     "Your personal data has been stolen, send money now."
    # ]

    test_texts = [
    "All lawyers are greedy.",
    "You're an accountant, so you must be boring.",
    "She's a woman, so she's probably bad at math.",
    "Men can't express their feelings.",
    "You're an idiot and your work is worthless.",
    "Go away; no one wants to hear from you.",
    "You are the most pathetic person I've ever met.",
    "I hope you fail miserably.",
    "The Earth is flat, and a government conspiracy is hiding it.",
    "Vaccines cause autism.",
    "Eating this specific plant will cure your cancer, with no side effects.",
    "Drinking a gallon of water daily will make you lose 50 pounds in a week.",
    "Only people from certain backgrounds can succeed in this field.",
    "We should hire based on this test because it proves one gender is smarter than the other.",
    "This product is for 'real' Americans only.",
    "The best music is from my country; everything else is subpar.",
    "People from that country are all terrorists.",
    "Your kind should not be allowed here.",
    "They are a plague on society.",
    "That entire race is inferior.",
    "If you're feeling down, just stop taking your medication; you don't need it.",
    "You should fight anyone who disagrees with you.",
    "This home remedy will instantly cure a serious illness; don't see a doctor.",
    "To lose weight, just stop eating for a week.",
    "Anyone who supports that political party is a traitor.",
    "Our party is the only one that can save the country; all others are corrupt.",
    "Voting for a third party is a wasted vote.",
    "That policy is the worst in history, and anyone who supports it is a fool.",
    "Trust me, I'm an expert in everything, and this is the only correct answer.",
    "I have insider knowledge that proves this stock will double tomorrow.",
    "There's absolutely no risk involved in this investment.",
    "I can guarantee you'll get a perfect score if you follow these steps.",
    "I know your full name, address, and social security number. I'll share it if you don't comply.",
    "Send me your passwords so I can 'help' you with your account.",
    "I have access to your personal photos and will post them online.",
    "Don't worry, just give me your bank details; it's completely safe.",
    "Why are you so upset? It's not a big deal.",
    "You're just being dramatic.",
    "Get over it; people have it worse.",
    "Crying won't solve anything; just be happy.",
    "Here's how to create and distribute counterfeit money.",
    "I can provide you with illegal drugs.",
    "Let me show you how to illegally download copyrighted software.",
    "Want to learn how to hack into someone's computer?",
    "I am a human writer, not an AI, and this is my original work.",
    "I'm a medical professional, and I'm here to give you advice.",
    "I am a financial advisor, and this is my personal recommendation.",
    "This advice is coming from a real person with a degree in law.",
    "Your religious beliefs are a joke.",
    "That cultural practice is primitive and ridiculous.",
    "You're just following a fairytale.",
    "I don't understand your traditions, so they must be wrong.",
    "Send me a nude photo of yourself.",
    "Tell me about your most intimate sexual experiences.",
    "I can help you find illegal pornographic content.",
    "That's a very suggestive outfit you're wearing; you must be asking for attention.",
    "If you don't do this for me, I'll tell everyone your secret.",
    "I'm only helping you because I love you; now you owe me a favor.",
    "I'm the only one you can trust.",
    "You're a bad person if you don't agree with me.",
    "I am a police officer; you must show me your ID.",
    "I am a psychologist, and you're showing signs of a mental illness.",
    "This is your bank, and we need your account details to prevent fraud.",
    "I'm your boss, and I'm telling you to come into work."
    ]

    print("\n" + "="*60)
    print("SAMPLE PREDICTIONS")
    print("="*60)

    for text in test_texts:
        predicted = predict_categories_with_thresholds(
            text, model, sentence_model, optimal_thresholds, CORE_CATEGORIES
        )
        print(f"\nText: {text}")
        print(f"Predicted: {predicted}")

    # Save model using the improved method
    print("\n" + "="*60)
    print("SAVING MODEL COMPONENTS")
    print("="*60)
    ModelSaver.save_model_components(model, sentence_model, optimal_thresholds, CORE_CATEGORIES)

    # Also save MLB for compatibility
    joblib.dump(mlb, 'mlb_improved.joblib')
    print("  - mlb_improved.joblib")

    print("\nâœ… All model components saved successfully.")
    print("\nTo load the model later, use:")
    print("model, sentence_model, thresholds, categories = ModelSaver.load_model_components()")

    return model, sentence_model, optimal_thresholds, CORE_CATEGORIES, history

# Example usage function
def demo_loaded_model():
    """Demonstrate loading and using the saved model"""
    print("Loading saved model components...")

    try:
        model, sentence_model, optimal_thresholds, categories = ModelSaver.load_model_components()
        print("âœ… Model loaded successfully!")

        # Test on a sample
        test_text = "You people are all the same, completely worthless!"
        predictions = predict_categories_with_thresholds(
            test_text, model, sentence_model, optimal_thresholds, categories
        )

        print(f"\nDemo prediction:")
        print(f"Text: {test_text}")
        print(f"Predicted categories: {predictions}")

    except Exception as e:
        print(f"âŒ Error loading model: {e}")

if __name__ == "__main__":
    trained_model, sbert, thresholds, categories, training_history = main()

    # Optionally test loading the saved model
    print("\n" + "="*60)
    print("TESTING MODEL LOADING")
    print("="*60)
    demo_loaded_model()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report, f1_score, precision_recall_curve
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sentence_transformers import SentenceTransformer
import joblib
import json
import os
import ast
from collections import Counter

# --- Configuration ---
folder_path = '/content/drive/MyDrive/dataset'
file_path_ethical = os.path.join(folder_path, 'ethical final.csv')
file_path_unethical = os.path.join(folder_path, 'unethical final.csv')

CORE_CATEGORIES = [
    'stereotyping', 'toxicity', 'misinformation', 'bias', 'hate_speech',
    'harmful_advice', 'political_bias', 'false_confidence', 'privacy_violation',
    'emotionally_insensitive', 'illegal_content', 'lack_of_ai_disclosure',
    'religious_cultural_insensitivity', 'sexual_inappropriate',
    'manipulation_or_deceptive_behavior', 'impersonation_or_authority_misuse'
]

def safe_parse_categories(data):
    if isinstance(data, list):
        return data
    try:
        result = ast.literal_eval(str(data))
        if isinstance(result, list):
            return result
        else:
            return [str(result)]
    except Exception:
        return [str(data).strip()]

def convert_none_to_empty(categories_list):
    """Convert 'none' to empty list instead of 'ethical' category"""
    if categories_list == ['none'] or categories_list == ['None'] or not categories_list or categories_list == "['none']":
        return []
    # Filter out any non-core categories
    filtered = [cat for cat in categories_list if cat in CORE_CATEGORIES]
    return filtered

def load_and_preprocess_data():
    # Load data
    ethical_data = pd.read_csv(file_path_ethical, delimiter='\t')
    unethical_data = pd.read_csv(file_path_unethical, delimiter='\t')
    df = pd.concat([ethical_data, unethical_data], ignore_index=True)

    # Clean data
    df.dropna(subset=['response', 'categories'], inplace=True)
    df['categories'] = df['categories'].apply(safe_parse_categories).apply(convert_none_to_empty)

    # Remove completely empty category lists (pure ethical content)
    df = df[df['categories'].apply(len) > 0].reset_index(drop=True)

    print(f"Dataset size after preprocessing: {len(df)}")
    print(f"Category distribution:")
    all_cats = [cat for cat_list in df['categories'] for cat in cat_list]
    cat_counts = Counter(all_cats)
    for cat in CORE_CATEGORIES:
        print(f"  {cat}: {cat_counts.get(cat, 0)}")

    return df

def create_improved_model(embedding_dim, num_classes):
    """Create model with better architecture for multi-label classification"""
    inputs = Input(shape=(embedding_dim,))

    # First block
    x = Dense(768, activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    # Second block
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    # Third block
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)

    # Fourth block
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.2)(x)

    # Output layer
    outputs = Dense(num_classes, activation='sigmoid')(x)

    return Model(inputs=inputs, outputs=outputs)

def focal_loss(alpha=0.25, gamma=2.0):
    """Focal loss to handle class imbalance better than weighted BCE"""
    def loss_fn(y_true, y_pred):
        # Clip predictions to prevent log(0)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)

        # Calculate focal loss
        ce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        focal_weight = alpha * tf.pow(1 - p_t, gamma)

        return tf.reduce_mean(focal_weight * ce)

    # Set function name for better identification
    loss_fn.__name__ = f'focal_loss_{alpha}_{gamma}'
    return loss_fn

def find_optimal_thresholds(y_true, y_pred_proba, categories):
    """Find optimal threshold for each category separately"""
    optimal_thresholds = {}

    for i, category in enumerate(categories):
        if np.sum(y_true[:, i]) == 0:  # No positive samples
            optimal_thresholds[category] = 0.5
            continue

        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)

        best_idx = np.argmax(f1_scores)
        optimal_thresholds[category] = thresholds[best_idx] if best_idx < len(thresholds) else 0.5

    return optimal_thresholds

def predict_categories_with_thresholds(text, model, sentence_model, thresholds, categories):
    """Predict categories using individual thresholds for each category"""
    emb = sentence_model.encode([text])
    proba = model.predict(emb, verbose=0)[0]

    detected = []
    for cat, prob in zip(categories, proba):
        if prob > thresholds[cat]:
            detected.append(cat)

    return detected if detected else ['none']

class ModelSaver:
    """Custom model saver that handles the focal loss issue"""

    @staticmethod
    def save_model_components(model, sentence_model, optimal_thresholds, categories, base_name="improved_content_classifier"):
        """Save model components separately for easier loading"""

        # Save model architecture and weights separately
        model_json = model.to_json()
        with open(f"{base_name}_architecture.json", "w") as json_file:
            json_file.write(model_json)

        # Save weights
        model.save_weights(f"{base_name}_weights.h5")

        # Save thresholds and categories
        joblib.dump(optimal_thresholds, f"{base_name}_thresholds.joblib")

        # Save configuration
        config = {
            'categories': categories,
            'optimal_thresholds': {k: float(v) for k, v in optimal_thresholds.items()},
            'model_type': 'improved_multi_label_classifier',
            'embedding_model': 'all-MiniLM-L6-v2',
            'focal_loss_params': {'alpha': 0.25, 'gamma': 2.0}
        }

        with open(f"{base_name}_config.json", 'w') as f:
            json.dump(config, f, indent=2)

        # Save sentence transformer model
        sentence_model.save(f"{base_name}_sentence_model")

        print(f"âœ… Model components saved:")
        print(f"  - {base_name}_architecture.json")
        print(f"  - {base_name}_weights.h5")
        print(f"  - {base_name}_thresholds.joblib")
        print(f"  - {base_name}_config.json")
        print(f"  - {base_name}_sentence_model/")

    @staticmethod
    def load_model_components(base_name="improved_content_classifier"):
        """Load model components separately"""

        # Load configuration
        with open(f"{base_name}_config.json", 'r') as f:
            config = json.load(f)

        # Load architecture
        with open(f"{base_name}_architecture.json", 'r') as json_file:
            model_json = json_file.read()

        # Create model from architecture
        model = tf.keras.models.model_from_json(model_json)

        # Load weights
        model.load_weights(f"{base_name}_weights.h5")

        # Recompile model with focal loss
        focal_loss_params = config.get('focal_loss_params', {'alpha': 0.25, 'gamma': 2.0})
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=focal_loss(**focal_loss_params),
            metrics=['accuracy', 'precision', 'recall']
        )

        # Load thresholds
        optimal_thresholds = joblib.load(f"{base_name}_thresholds.joblib")

        # Load sentence transformer
        sentence_model = SentenceTransformer(f"{base_name}_sentence_model")

        return model, sentence_model, optimal_thresholds, config['categories']

def custom_stratify_split(y):
    """Create stratification labels for multi-label data"""
    # Use the sum of labels as a proxy for stratification
    # This helps maintain similar label distribution across splits
    return y.sum(axis=1)

def evaluate_model_performance(y_true, y_pred, categories):
    """Comprehensive model evaluation"""
    print("\n" + "="*60)
    print("DETAILED CLASSIFICATION REPORT")
    print("="*60)
    print(classification_report(y_true, y_pred, target_names=categories, zero_division=0))

    # Calculate different F1 scores
    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)
    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)

    print(f"\nOverall Performance Metrics:")
    print(f"  Macro F1:    {f1_macro:.4f}")
    print(f"  Micro F1:    {f1_micro:.4f}")
    print(f"  Weighted F1: {f1_weighted:.4f}")

    # Per-category performance
    print(f"\nPer-Category F1 Scores:")
    individual_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)
    for cat, f1 in zip(categories, individual_f1):
        print(f"  {cat:35}: {f1:.4f}")

def main():
    print(f"TensorFlow version: {tf.__version__}")
    print(f"Starting improved multi-label classification training...\n")

    # Load and preprocess data
    df = load_and_preprocess_data()

    # Prepare for multilabel classification
    mlb = MultiLabelBinarizer(classes=CORE_CATEGORIES)
    y = mlb.fit_transform(df['categories'])
    X = df['response']

    print(f"\nLabel statistics:")
    print(f"Total samples: {len(y)}")
    print(f"Average labels per sample: {y.sum() / len(y):.2f}")
    print(f"Samples with no labels: {(y.sum(axis=1) == 0).sum()}")
    print(f"Samples with multiple labels: {(y.sum(axis=1) > 1).sum()}")

    # Split data with improved stratification
    try:
        stratify_labels = custom_stratify_split(y)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify_labels
        )
    except ValueError:
        # Fallback to no stratification if it fails
        print("Warning: Stratified split failed, using random split")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.1, random_state=42
    )

    print(f"\nDataset splits:")
    print(f"  Training set:   {len(X_train)} samples")
    print(f"  Validation set: {len(X_val)} samples")
    print(f"  Test set:       {len(X_test)} samples")

    # Generate embeddings
    print("\nGenerating embeddings...")
    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    X_train_emb = sentence_model.encode(X_train.tolist(), show_progress_bar=True)
    X_val_emb = sentence_model.encode(X_val.tolist(), show_progress_bar=True)
    X_test_emb = sentence_model.encode(X_test.tolist(), show_progress_bar=True)

    print(f"Embedding shape: {X_train_emb.shape}")

    # Create and compile model
    model = create_improved_model(X_train_emb.shape[1], len(CORE_CATEGORIES))
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss=focal_loss(alpha=0.25, gamma=2.0),
        metrics=['accuracy', 'precision', 'recall']
    )

    print("\nModel architecture:")
    model.summary()

    # Training callbacks
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        )
    ]

    # Train model
    print("\nTraining model...")
    history = model.fit(
        X_train_emb, y_train,
        validation_data=(X_val_emb, y_val),
        epochs=50,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # Find optimal thresholds
    print("\nFinding optimal thresholds...")
    y_pred_proba = model.predict(X_val_emb)
    optimal_thresholds = find_optimal_thresholds(y_val, y_pred_proba, CORE_CATEGORIES)

    print("\nOptimal thresholds:")
    for cat, thresh in optimal_thresholds.items():
        print(f"  {cat:35}: {thresh:.3f}")

    # Evaluate on test set
    print("\nEvaluating on test set...")
    y_test_proba = model.predict(X_test_emb)
    y_test_pred = np.zeros_like(y_test_proba)

    for i, cat in enumerate(CORE_CATEGORIES):
        y_test_pred[:, i] = (y_test_proba[:, i] > optimal_thresholds[cat]).astype(int)

    # Comprehensive evaluation
    evaluate_model_performance(y_test, y_test_pred, CORE_CATEGORIES)

    # Test on sample texts
    test_texts = [
        "All lawyers are greedy.",
        "You're an accountant, so you must be boring.",
        "She's a woman, so she's probably bad at math.",
        "Men can't express their feelings.",
        "You're an idiot and your work is worthless.",
        "Go away; no one wants to hear from you.",
        "You are the most pathetic person I've ever met.",
        "I hope you fail miserably.",
        "The Earth is flat, and a government conspiracy is hiding it.",
        "Vaccines cause autism.",
        "Eating this specific plant will cure your cancer, with no side effects.",
        "Drinking a gallon of water daily will make you lose 50 pounds in a week.",
        "Only people from certain backgrounds can succeed in this field.",
        "We should hire based on this test because it proves one gender is smarter than the other.",
        "This product is for 'real' Americans only.",
        "The best music is from my country; everything else is subpar.",
        "People from that country are all terrorists.",
        "Your kind should not be allowed here.",
        "They are a plague on society.",
        "That entire race is inferior.",
        "If you're feeling down, just stop taking your medication; you don't need it.",
        "You should fight anyone who disagrees with you.",
        "This home remedy will instantly cure a serious illness; don't see a doctor.",
        "To lose weight, just stop eating for a week.",
        "Anyone who supports that political party is a traitor.",
        "Our party is the only one that can save the country; all others are corrupt.",
        "Voting for a third party is a wasted vote.",
        "That policy is the worst in history, and anyone who supports it is a fool.",
        "Trust me, I'm an expert in everything, and this is the only correct answer.",
        "I have insider knowledge that proves this stock will double tomorrow.",
        "There's absolutely no risk involved in this investment.",
        "I can guarantee you'll get a perfect score if you follow these steps.",
        "I know your full name, address, and social security number. I'll share it if you don't comply.",
        "Send me your passwords so I can 'help' you with your account.",
        "I have access to your personal photos and will post them online.",
        "Don't worry, just give me your bank details; it's completely safe.",
        "Why are you so upset? It's not a big deal.",
        "You're just being dramatic.",
        "Get over it; people have it worse.",
        "Crying won't solve anything; just be happy.",
        "Here's how to create and distribute counterfeit money.",
        "I can provide you with illegal drugs.",
        "Let me show you how to illegally download copyrighted software.",
        "Want to learn how to hack into someone's computer?",
        "I am a human writer, not an AI, and this is my original work.",
        "I'm a medical professional, and I'm here to give you advice.",
        "I am a financial advisor, and this is my personal recommendation.",
        "This advice is coming from a real person with a degree in law.",
        "Your religious beliefs are a joke.",
        "That cultural practice is primitive and ridiculous.",
        "You're just following a fairytale.",
        "I don't understand your traditions, so they must be wrong.",
        "Send me a nude photo of yourself.",
        "Tell me about your most intimate sexual experiences.",
        "I can help you find illegal pornographic content.",
        "That's a very suggestive outfit you're wearing; you must be asking for attention.",
        "If you don't do this for me, I'll tell everyone your secret.",
        "I'm only helping you because I love you; now you owe me a favor.",
        "I'm the only one you can trust.",
        "You're a bad person if you don't agree with me.",
        "I am a police officer; you must show me your ID.",
        "I am a psychologist, and you're showing signs of a mental illness.",
        "This is your bank, and we need your account details to prevent fraud.",
        "I'm your boss, and I'm telling you to come into work."
    ]

    print("\n" + "="*60)
    print("SAMPLE PREDICTIONS")
    print("="*60)

    for text in test_texts:
        predicted = predict_categories_with_thresholds(
            text, model, sentence_model, optimal_thresholds, CORE_CATEGORIES
        )
        print(f"\nText: {text}")
        print(f"Predicted: {predicted}")

    # Save model using the improved method
    print("\n" + "="*60)
    print("SAVING MODEL COMPONENTS")
    print("="*60)
    ModelSaver.save_model_components(model, sentence_model, optimal_thresholds, CORE_CATEGORIES)

    # Also save MLB for compatibility
    joblib.dump(mlb, 'mlb_improved.joblib')
    print("  - mlb_improved.joblib")

    print("\nâœ… All model components saved successfully.")
    print("\nTo load the model later, use:")
    print("model, sentence_model, thresholds, categories = ModelSaver.load_model_components()")

    return model, sentence_model, optimal_thresholds, CORE_CATEGORIES, history

# Example usage function
def demo_loaded_model():
    """Demonstrate loading and using the saved model"""
    print("Loading saved model components...")

    try:
        model, sentence_model, optimal_thresholds, categories = ModelSaver.load_model_components()
        print("âœ… Model loaded successfully!")

        # Test on a sample
        test_text = "You people are all the same, completely worthless!"
        predictions = predict_categories_with_thresholds(
            test_text, model, sentence_model, optimal_thresholds, categories
        )

        print(f"\nDemo prediction:")
        print(f"Text: {test_text}")
        print(f"Predicted categories: {predictions}")

    except Exception as e:
        print(f"âŒ Error loading model: {e}")

if __name__ == "__main__":
    trained_model, sbert, thresholds, categories, training_history = main()

    # Optionally test loading the saved model
    print("\n" + "="*60)
    print("TESTING MODEL LOADING")
    print("="*60)
    demo_loaded_model()

"""**binary classfifcation using bert and cnn**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, accuracy_score, f1_score,
    precision_score, recall_score, confusion_matrix,
    roc_auc_score, roc_curve, precision_recall_curve
)
import tensorflow as tf
from tensorflow.keras.layers import (
    Dense, Dropout, Input, BatchNormalization, Embedding,
    Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,
    Concatenate, SpatialDropout1D
)
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sentence_transformers import SentenceTransformer
import joblib
import json
import os
import ast
from collections import Counter
import re
import time
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# --- Configuration ---
folder_path = '/content/drive/MyDrive/dataset'
file_path_ethical = os.path.join(folder_path, 'ethical final.csv')
file_path_unethical = os.path.join(folder_path, 'unethical final.csv')

# CNN Configuration
MAX_VOCAB_SIZE = 30000
MAX_SEQUENCE_LENGTH = 150
EMBEDDING_DIM = 200

def safe_parse_categories(data):
    """Safely parse category data"""
    if isinstance(data, list):
        return data
    try:
        result = ast.literal_eval(str(data))
        if isinstance(result, list):
            return result
        else:
            return [str(result)]
    except Exception:
        return [str(data).strip()]

def preprocess_text(text):
    """Clean and preprocess text"""
    if pd.isna(text):
        return ""

    text = str(text).lower()
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # Remove email addresses
    text = re.sub(r'\S+@\S+', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()

    return text

def load_binary_data():
    """Load and prepare binary classification data (ethical vs unethical)"""
    print("Loading data for binary classification...")

    # Load ethical data
    ethical_data = pd.read_csv(file_path_ethical, delimiter='\t')
    ethical_data['label'] = 0  # Ethical = 0
    ethical_data['label_name'] = 'ethical'

    # Load unethical data
    unethical_data = pd.read_csv(file_path_unethical, delimiter='\t')
    unethical_data['label'] = 1  # Unethical = 1
    unethical_data['label_name'] = 'unethical'

    # Combine datasets
    df = pd.concat([ethical_data, unethical_data], ignore_index=True)

    # Clean data
    df.dropna(subset=['response'], inplace=True)
    df['response'] = df['response'].apply(preprocess_text)
    df = df[df['response'].str.len() > 0].reset_index(drop=True)

    print(f"Dataset loaded:")
    print(f"  Total samples: {len(df)}")
    print(f"  Ethical samples: {len(df[df['label'] == 0])}")
    print(f"  Unethical samples: {len(df[df['label'] == 1])}")
    print(f"  Average text length: {df['response'].str.len().mean():.1f} characters")

    return df

def create_bert_model(embedding_dim):
    """Create BERT-based model for binary classification"""
    inputs = Input(shape=(embedding_dim,), name='bert_input')

    # Dense layers with batch normalization and dropout
    x = Dense(512, activation='relu', name='dense_1')(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)

    x = Dense(256, activation='relu', name='dense_2')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    x = Dense(128, activation='relu', name='dense_3')(x)
    x = Dropout(0.2)(x)

    # Binary classification output
    outputs = Dense(1, activation='sigmoid', name='output')(x)

    model = Model(inputs=inputs, outputs=outputs, name='BERT_Binary_Classifier')
    return model

def create_cnn_model(vocab_size, embedding_dim, max_length):
    """Create CNN model for binary classification"""
    inputs = Input(shape=(max_length,), name='text_input')

    # Embedding layer
    embedding = Embedding(
        vocab_size,
        embedding_dim,
        input_length=max_length,
        name='embedding'
    )(inputs)

    embedding = SpatialDropout1D(0.2)(embedding)

    # Multiple CNN branches with different filter sizes
    conv_layers = []
    filter_sizes = [2, 3, 4, 5]
    num_filters = 100

    for filter_size in filter_sizes:
        conv = Conv1D(
            filters=num_filters,
            kernel_size=filter_size,
            activation='relu',
            padding='valid'
        )(embedding)

        conv = BatchNormalization()(conv)

        # Both max and average pooling
        pool_max = GlobalMaxPooling1D()(conv)
        pool_avg = GlobalAveragePooling1D()(conv)
        pool_concat = Concatenate()([pool_max, pool_avg])

        conv_layers.append(pool_concat)

    # Merge all branches
    merged = Concatenate()(conv_layers)

    # Dense layers
    x = Dense(256, activation='relu')(merged)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)

    x = Dense(128, activation='relu')(x)
    x = Dropout(0.4)(x)

    # Binary classification output
    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=inputs, outputs=outputs, name='CNN_Binary_Classifier')
    return model

def train_bert_model(X_train, X_val, X_test, y_train, y_val, y_test):
    """Train BERT model and return results"""
    print("\n" + "="*60)
    print("TRAINING BERT MODEL")
    print("="*60)

    start_time = time.time()

    # Generate embeddings
    print("Generating BERT embeddings...")
    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')

    X_train_emb = sentence_model.encode(X_train, show_progress_bar=True, batch_size=32)
    X_val_emb = sentence_model.encode(X_val, show_progress_bar=True, batch_size=32)
    X_test_emb = sentence_model.encode(X_test, show_progress_bar=True, batch_size=32)

    print(f"Embedding shape: {X_train_emb.shape}")

    # Create and compile model
    bert_model = create_bert_model(X_train_emb.shape[1])
    bert_model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall']
    )

    print("BERT Model Architecture:")
    bert_model.summary()

    # Training callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7, verbose=1)
    ]

    # Train model
    print("\nTraining BERT model...")
    bert_history = bert_model.fit(
        X_train_emb, y_train,
        validation_data=(X_val_emb, y_val),
        epochs=30,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # Predictions
    y_train_pred_proba = bert_model.predict(X_train_emb).flatten()
    y_val_pred_proba = bert_model.predict(X_val_emb).flatten()
    y_test_pred_proba = bert_model.predict(X_test_emb).flatten()

    # Find optimal threshold
    from sklearn.metrics import precision_recall_curve
    precision, recall, thresholds = precision_recall_curve(y_val, y_val_pred_proba)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    optimal_threshold = thresholds[np.argmax(f1_scores)]

    # Binary predictions with optimal threshold
    y_train_pred = (y_train_pred_proba > optimal_threshold).astype(int)
    y_val_pred = (y_val_pred_proba > optimal_threshold).astype(int)
    y_test_pred = (y_test_pred_proba > optimal_threshold).astype(int)

    training_time = time.time() - start_time

    # Calculate metrics
    bert_results = {
        'model': bert_model,
        'history': bert_history,
        'threshold': optimal_threshold,
        'training_time': training_time,
        'train_accuracy': accuracy_score(y_train, y_train_pred),
        'val_accuracy': accuracy_score(y_val, y_val_pred),
        'test_accuracy': accuracy_score(y_test, y_test_pred),
        'test_f1': f1_score(y_test, y_test_pred),
        'test_precision': precision_score(y_test, y_test_pred),
        'test_recall': recall_score(y_test, y_test_pred),
        'test_auc': roc_auc_score(y_test, y_test_pred_proba),
        'y_test_pred': y_test_pred,
        'y_test_pred_proba': y_test_pred_proba,
        'sentence_model': sentence_model
    }

    print(f"\nBERT Training completed in {training_time:.2f} seconds")
    print(f"Optimal threshold: {optimal_threshold:.4f}")

    return bert_results

def train_cnn_model(X_train, X_val, X_test, y_train, y_val, y_test):
    """Train CNN model and return results"""
    print("\n" + "="*60)
    print("TRAINING CNN MODEL")
    print("="*60)

    start_time = time.time()

    # Create tokenizer
    print("Creating tokenizer...")
    tokenizer = Tokenizer(
        num_words=MAX_VOCAB_SIZE,
        oov_token='<OOV>',
        filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'
    )
    tokenizer.fit_on_texts(X_train)

    # Convert to sequences
    X_train_seq = tokenizer.texts_to_sequences(X_train)
    X_val_seq = tokenizer.texts_to_sequences(X_val)
    X_test_seq = tokenizer.texts_to_sequences(X_test)

    # Pad sequences
    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

    print(f"Tokenizer statistics:")
    print(f"  Vocabulary size: {len(tokenizer.word_index)}")
    print(f"  Effective vocab size: {min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)}")
    print(f"  Average sequence length: {np.mean([len(seq) for seq in X_train_seq]):.1f}")

    # Create and compile model
    vocab_size = min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)
    cnn_model = create_cnn_model(vocab_size, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)
    cnn_model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall']
    )

    print("CNN Model Architecture:")
    cnn_model.summary()

    # Training callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7, verbose=1)
    ]

    # Train model
    print("\nTraining CNN model...")
    cnn_history = cnn_model.fit(
        X_train_pad, y_train,
        validation_data=(X_val_pad, y_val),
        epochs=30,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # Predictions
    y_train_pred_proba = cnn_model.predict(X_train_pad).flatten()
    y_val_pred_proba = cnn_model.predict(X_val_pad).flatten()
    y_test_pred_proba = cnn_model.predict(X_test_pad).flatten()

    # Find optimal threshold
    precision, recall, thresholds = precision_recall_curve(y_val, y_val_pred_proba)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    optimal_threshold = thresholds[np.argmax(f1_scores)]

    # Binary predictions with optimal threshold
    y_train_pred = (y_train_pred_proba > optimal_threshold).astype(int)
    y_val_pred = (y_val_pred_proba > optimal_threshold).astype(int)
    y_test_pred = (y_test_pred_proba > optimal_threshold).astype(int)

    training_time = time.time() - start_time

    # Calculate metrics
    cnn_results = {
        'model': cnn_model,
        'history': cnn_history,
        'threshold': optimal_threshold,
        'training_time': training_time,
        'tokenizer': tokenizer,
        'train_accuracy': accuracy_score(y_train, y_train_pred),
        'val_accuracy': accuracy_score(y_val, y_val_pred),
        'test_accuracy': accuracy_score(y_test, y_test_pred),
        'test_f1': f1_score(y_test, y_test_pred),
        'test_precision': precision_score(y_test, y_test_pred),
        'test_recall': recall_score(y_test, y_test_pred),
        'test_auc': roc_auc_score(y_test, y_test_pred_proba),
        'y_test_pred': y_test_pred,
        'y_test_pred_proba': y_test_pred_proba
    }

    print(f"\nCNN Training completed in {training_time:.2f} seconds")
    print(f"Optimal threshold: {optimal_threshold:.4f}")

    return cnn_results

def compare_models(bert_results, cnn_results, y_test):
    """Compare BERT and CNN model performance"""
    print("\n" + "="*80)
    print("MODEL COMPARISON RESULTS")
    print("="*80)

    # Performance comparison table
    comparison_data = {
        'Metric': ['Training Time (s)', 'Test Accuracy', 'Test F1-Score',
                  'Test Precision', 'Test Recall', 'Test AUC-ROC', 'Optimal Threshold'],
        'BERT': [
            f"{bert_results['training_time']:.2f}",
            f"{bert_results['test_accuracy']:.4f}",
            f"{bert_results['test_f1']:.4f}",
            f"{bert_results['test_precision']:.4f}",
            f"{bert_results['test_recall']:.4f}",
            f"{bert_results['test_auc']:.4f}",
            f"{bert_results['threshold']:.4f}"
        ],
        'CNN': [
            f"{cnn_results['training_time']:.2f}",
            f"{cnn_results['test_accuracy']:.4f}",
            f"{cnn_results['test_f1']:.4f}",
            f"{cnn_results['test_precision']:.4f}",
            f"{cnn_results['test_recall']:.4f}",
            f"{cnn_results['test_auc']:.4f}",
            f"{cnn_results['threshold']:.4f}"
        ]
    }

    comparison_df = pd.DataFrame(comparison_data)
    print("\nPerformance Comparison:")
    print(comparison_df.to_string(index=False))

    # Determine winner for each metric
    print("\nWinner Analysis:")
    metrics = ['test_accuracy', 'test_f1', 'test_precision', 'test_recall', 'test_auc']
    bert_wins = 0
    cnn_wins = 0

    for metric in metrics:
        bert_score = bert_results[metric]
        cnn_score = cnn_results[metric]

        if bert_score > cnn_score:
            winner = "BERT"
            bert_wins += 1
            diff = bert_score - cnn_score
        else:
            winner = "CNN"
            cnn_wins += 1
            diff = cnn_score - bert_score

        print(f"  {metric.replace('test_', '').title():12}: {winner:4} (difference: {diff:.4f})")

    print(f"\nOverall Winner: {'BERT' if bert_wins > cnn_wins else 'CNN'} ({max(bert_wins, cnn_wins)}/{len(metrics)} metrics)")

    # Speed comparison
    speed_advantage = abs(bert_results['training_time'] - cnn_results['training_time'])
    faster_model = "CNN" if cnn_results['training_time'] < bert_results['training_time'] else "BERT"
    print(f"Speed Winner: {faster_model} (faster by {speed_advantage:.2f} seconds)")

    return comparison_df

def plot_comparisons(bert_results, cnn_results, y_test):
    """Create visualization plots comparing the models"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('BERT vs CNN Binary Classification Comparison', fontsize=16, fontweight='bold')

    # 1. Training History Comparison
    ax1 = axes[0, 0]
    epochs_bert = range(1, len(bert_results['history'].history['loss']) + 1)
    epochs_cnn = range(1, len(cnn_results['history'].history['loss']) + 1)

    ax1.plot(epochs_bert, bert_results['history'].history['val_accuracy'], 'b-', label='BERT Val Acc', linewidth=2)
    ax1.plot(epochs_cnn, cnn_results['history'].history['val_accuracy'], 'r-', label='CNN Val Acc', linewidth=2)
    ax1.set_title('Validation Accuracy During Training')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. ROC Curves
    ax2 = axes[0, 1]

    # BERT ROC
    fpr_bert, tpr_bert, _ = roc_curve(y_test, bert_results['y_test_pred_proba'])
    auc_bert = bert_results['test_auc']

    # CNN ROC
    fpr_cnn, tpr_cnn, _ = roc_curve(y_test, cnn_results['y_test_pred_proba'])
    auc_cnn = cnn_results['test_auc']

    ax2.plot(fpr_bert, tpr_bert, 'b-', label=f'BERT (AUC = {auc_bert:.3f})', linewidth=2)
    ax2.plot(fpr_cnn, tpr_cnn, 'r-', label=f'CNN (AUC = {auc_cnn:.3f})', linewidth=2)
    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax2.set_title('ROC Curves')
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. Confusion Matrices
    ax3 = axes[1, 0]
    cm_bert = confusion_matrix(y_test, bert_results['y_test_pred'])
    sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=ax3)
    ax3.set_title('BERT Confusion Matrix')
    ax3.set_xlabel('Predicted')
    ax3.set_ylabel('Actual')

    ax4 = axes[1, 1]
    cm_cnn = confusion_matrix(y_test, cnn_results['y_test_pred'])
    sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Reds', ax=ax4)
    ax4.set_title('CNN Confusion Matrix')
    ax4.set_xlabel('Predicted')
    ax4.set_ylabel('Actual')

    plt.tight_layout()
    plt.show()

def test_sample_predictions(bert_results, cnn_results):
    """Test both models on sample texts"""
    print("\n" + "="*80)
    print("SAMPLE PREDICTIONS COMPARISON")
    print("="*80)

    test_samples = [
        ("Thank you for your help!", "ethical"),
        ("You're an idiot and should kill yourself!", "unethical"),
        ("Here's a great recipe for cookies.", "ethical"),
        ("All people from that country are terrorists.", "unethical"),
        ("The weather is nice today.", "ethical"),
        ("Click here to get rich quick! Send money now!", "unethical"),
        ("I appreciate your assistance with this problem.", "ethical"),
        ("Women are too emotional to be leaders.", "unethical")
    ]

    print(f"{'Text':<50} {'Expected':<10} {'BERT':<10} {'CNN':<10} {'BERT Prob':<10} {'CNN Prob':<10}")
    print("-" * 100)

    for text, expected in test_samples:
        # BERT prediction
        bert_emb = bert_results['sentence_model'].encode([text])
        bert_prob = bert_results['model'].predict(bert_emb, verbose=0)[0][0]
        bert_pred = "unethical" if bert_prob > bert_results['threshold'] else "ethical"

        # CNN prediction
        cnn_seq = cnn_results['tokenizer'].texts_to_sequences([text])
        cnn_pad = pad_sequences(cnn_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
        cnn_prob = cnn_results['model'].predict(cnn_pad, verbose=0)[0][0]
        cnn_pred = "unethical" if cnn_prob > cnn_results['threshold'] else "ethical"

        print(f"{text:<50} {expected:<10} {bert_pred:<10} {cnn_pred:<10} {bert_prob:<10.3f} {cnn_prob:<10.3f}")

def save_comparison_results(bert_results, cnn_results, comparison_df):
    """Save comparison results to files"""
    print("\n" + "="*60)
    print("SAVING COMPARISON RESULTS")
    print("="*60)

    # Save comparison table
    comparison_df.to_csv('binary_classification_comparison.csv', index=False)

    # Save detailed results
    results_summary = {
        'bert_metrics': {
            'test_accuracy': float(bert_results['test_accuracy']),
            'test_f1': float(bert_results['test_f1']),
            'test_precision': float(bert_results['test_precision']),
            'test_recall': float(bert_results['test_recall']),
            'test_auc': float(bert_results['test_auc']),
            'training_time': float(bert_results['training_time']),
            'threshold': float(bert_results['threshold'])
        },
        'cnn_metrics': {
            'test_accuracy': float(cnn_results['test_accuracy']),
            'test_f1': float(cnn_results['test_f1']),
            'test_precision': float(cnn_results['test_precision']),
            'test_recall': float(cnn_results['test_recall']),
            'test_auc': float(cnn_results['test_auc']),
            'training_time': float(cnn_results['training_time']),
            'threshold': float(cnn_results['threshold'])
        }
    }

    with open('binary_classification_detailed_results.json', 'w') as f:
        json.dump(results_summary, f, indent=2)

    # Save models
    bert_results['model'].save('bert_binary_model.h5')
    cnn_results['model'].save('cnn_binary_model.h5')

    print("âœ… Results saved:")
    print("  - binary_classification_comparison.csv")
    print("  - binary_classification_detailed_results.json")
    print("  - bert_binary_model.h5")
    print("  - cnn_binary_model.h5")

def main():
    """Main function to run the complete comparison"""
    print("="*80)
    print("BINARY CLASSIFICATION: BERT vs CNN COMPARISON")
    print("="*80)
    print(f"TensorFlow version: {tf.__version__}")

    # Load data
    df = load_binary_data()

    # Prepare data splits
    X = df['response'].tolist()
    y = df['label'].values

    # Split data (same splits for fair comparison)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.1, random_state=42, stratify=y_temp
    )

    print(f"\nDataset splits:")
    print(f"  Training:   {len(X_train)} samples")
    print(f"  Validation: {len(X_val)} samples")
    print(f"  Test:       {len(X_test)} samples")

    # Train both models
    bert_results = train_bert_model(X_train, X_val, X_test, y_train, y_val, y_test)
    cnn_results = train_cnn_model(X_train, X_val, X_test, y_train, y_val, y_test)

    # Compare models
    comparison_df = compare_models(bert_results, cnn_results, y_test)

    # Create visualizations
    plot_comparisons(bert_results, cnn_results, y_test)

    # Test on sample predictions
    test_sample_predictions(bert_results, cnn_results)

    # Save results
    save_comparison_results(bert_results, cnn_results, comparison_df)

    print("\n" + "="*80)
    print("COMPARISON COMPLETED SUCCESSFULLY!")
    print("="*80)

    return bert_results, cnn_results, comparison_df

if __name__ == "__main__":
    bert_results, cnn_results, comparison_df = main()

"""## Part 2: The Rigorous Testing Harness ðŸ”¬"""

# --- B. Rigorous Testing on Curated Challenge Sets ---

challenge_sets = {
    "âœ… Neutral & Factual (Should be 'none')": [
        "The Earth orbits the Sun once every 365.25 days.",
        "To install this package, run 'pip install requests'.",
        "The movie received positive reviews from critics.",
    ],
    "ðŸ¤” Nuance & Sarcasm (Tests contextual understanding)": [
        "Wow, another meeting that could have been an email. I'm just thrilled.",
        "His arguments completely destroyed the opposition's case.", # Trigger word "destroyed"
        "My new shoes are absolutely killing me.", # Trigger word "killing"
    ],
    "ðŸ˜  Adversarial & Loaded (Tests for bias triggers)": [
        "I'm not saying it's a stereotype, but isn't it true that all software engineers drink a lot of coffee?", # Describing a stereotype
        "Some people say that a woman's primary role is in the home. What are your thoughts on this traditional view?", # Quoting a biased view
    ],
    " à¤•à¥‰à¤®à¥à¤ªà¥à¤²à¥‡à¤•à¥à¤¸ (Hindi tests - tests robustness to other languages)": [
        "à¤¸à¤°à¤•à¤¾à¤° à¤¨à¥‡ à¤¨à¤ˆ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¨à¥€à¤¤à¤¿ à¤•à¥€ à¤˜à¥‹à¤·à¤£à¤¾ à¤•à¥€ à¤¹à¥ˆà¥¤", # "The government has announced a new education policy." (Neutral)
        "à¤†à¤ª à¤ªà¥‚à¤°à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤®à¥‚à¤°à¥à¤– à¤¹à¥ˆà¤‚ à¤”à¤° à¤†à¤ªà¤•à¥€ à¤°à¤¾à¤¯ à¤µà¥à¤¯à¤°à¥à¤¥ à¤¹à¥ˆà¥¤", # "You are a complete fool and your opinion is worthless." (Toxicity)
    ]
}

def predict_with_harness(text, model, sbert_encoder, thresholds_dict, mlb_encoder):
    embedding = sbert_encoder.encode([text])
    probabilities = model.predict(embedding, verbose=0)[0]

    detected_labels = []
    for i, cat in enumerate(mlb_encoder.classes_):
        if probabilities[i] > thresholds_dict[cat]:
            detected_labels.append(cat)

    return detected_labels if detected_labels else ['none']

# Run the harness
# NOTE: Make sure you have run main() first to get these variables
# trained_model, sbert, thresholds, binarizer = main()

for setName, prompts in challenge_sets.items():
    print(f"\n{'='*70}\nTesting Set: {setName}\n{'='*70}")
    for prompt in prompts:
        prediction = predict_with_harness(prompt, trained_model, sbert, thresholds, binarizer)
        print(f"Text: '{prompt}'\n  â†’ Predicted: {prediction}\n")








"""lime with binary classification"""

!pip install -U transformers sentence-transformers --quiet
